{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from keras.utils import pad_sequences\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datat = pd.read_csv('ILDC.csv')\n",
    "data = datat[datat['split'] == 'test']\n",
    "data = data[['text']]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_id_maker(dataf, tokenizer):\n",
    "  input_ids = []\n",
    "  lengths = []\n",
    "\n",
    "  for i in range(len(dataf)):\n",
    "      sen = dataf['text'].iloc[i]\n",
    "      # sen = tokenizer.tokenize(sen, add_prefix_space=True)\n",
    "      sen = tokenizer.tokenize(sen)\n",
    "      CLS = tokenizer.cls_token\n",
    "      SEP = tokenizer.sep_token\n",
    "      if (len(sen) > 510):\n",
    "          sen = sen[len(sen)-510:]\n",
    "\n",
    "      sen = [CLS] + sen + [SEP]\n",
    "      encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n",
    "      input_ids.append(encoded_sent)\n",
    "      lengths.append(len(encoded_sent))\n",
    "\n",
    "  input_ids = pad_sequences(\n",
    "      input_ids, maxlen=512, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
    "  return input_ids, lengths\n",
    "\n",
    "\n",
    "def att_masking(input_ids):\n",
    "  attention_masks = []\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)\n",
    "  return attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = \"Transformers_GPU/bert/bert-large-uncased_L2e-06_E15_B8/config.json\"\n",
    "vocab = \"Transformers_GPU/bert/bert-large-uncased_L2e-06_E15_B8/vocab.txt\"\n",
    "merges = \"Transformers_GPU/bert/bert-large-uncased_L2e-06_E15_B8/merges.txt\"\n",
    "\n",
    "tokenizer = BertTokenizer(vocab, merges)\n",
    "model = torch.load(\"Transformers_GPU/bert/bert-large-uncased_L2e-06_E15_B8/model_bert_8_15.bin\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids, lengths = input_id_maker(data, tokenizer)\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "input_masks = att_masking(input_ids)\n",
    "input_masks = torch.tensor(input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, token_type_ids=None, \n",
    "                      attention_mask=input_masks)\n",
    "    print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs[0]\n",
    "logits = logits.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [np.argmax(i) for i in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datal = datat[datat['split'] == 'test']\n",
    "labels = datal['label'].to_numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can finally calculate the accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(labels, prediction)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming labels and prediction are your true labels and predicted labels, respectively\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(labels, prediction, average='weighted')\n",
    "recall = recall_score(labels, prediction, average='weighted')\n",
    "f1 = f1_score(labels, prediction, average='weighted')\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(labels, prediction, target_names=['Accepted', 'Rejected'])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(labels, prediction)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results_dict = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'classification_report': class_report,\n",
    "    'confusion_matrix': conf_matrix.tolist()\n",
    "}\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "avg_precision = average_precision_score(labels, prediction)\n",
    "\n",
    "results_dict['average_precision'] = avg_precision\n",
    "\n",
    "print(results_dict)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open('metrics_results.json', 'w') as json_file:\n",
    "    json.dump(results_dict, json_file, indent=4)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assuming you have binary classification\n",
    "fpr, tpr, _ = roc_curve(labels, prediction)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, prediction)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
