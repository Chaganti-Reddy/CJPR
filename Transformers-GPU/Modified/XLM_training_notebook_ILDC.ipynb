{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9Ur0UDTmP8H"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers Keras-Preprocessing wandb pytorch-lightning sacremoses sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBxzOic27JbH"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "key=\"d75571bf9259088cd0a735d5f9e10de08e105a99\"\n",
    "wandb.login(key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HWslNrUJdla"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import textwrap\n",
    "import progressbar\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.metrics import (accuracy_score, f1_score, recall_score, precision_score, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBjxqwueJ0pq"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('ILDC.csv') # path to multi_dataset\n",
    "train_set = df.query(\" split=='train' \")\n",
    "test_set = df.query(\" split=='test' \")\n",
    "validation_set = df.query(\" split=='dev' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7Q9PNnweVuf"
   },
   "outputs": [],
   "source": [
    "# load all models and select roberta\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    'xlm': (XLMForSequenceClassification, XLMRobertaTokenizer, XLMRobertaConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
    "\n",
    "model_type = 'xlm' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
    "model_name = 'xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb-lpoe2QiHL"
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqBkaIbDKMlX"
   },
   "outputs": [],
   "source": [
    "def input_id_maker(dataf, tokenizer):\n",
    "  input_ids = []\n",
    "  lengths = []\n",
    "\n",
    "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
    "    sen = dataf['text'].iloc[i]\n",
    "    sen = tokenizer.tokenize(sen)\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    if(len(sen) > 510):\n",
    "      sen = sen[len(sen)-510:]\n",
    "\n",
    "    sen = [CLS] + sen + [SEP]\n",
    "    encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n",
    "    input_ids.append(encoded_sent)\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "  input_ids = pad_sequences(input_ids, maxlen=512, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
    "  return input_ids, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357711,
     "status": "ok",
     "timestamp": 1703324589511,
     "user": {
      "displayName": "Chaganti Ramireddy",
      "userId": "07233275673922063388"
     },
     "user_tz": -330
    },
    "id": "ctB5AycbKVj6"
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_lengths = input_id_maker(train_set, tokenizer)\n",
    "validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1703324589513,
     "user": {
      "displayName": "Chaganti Ramireddy",
      "userId": "07233275673922063388"
     },
     "user_tz": -330
    },
    "id": "c_rrz5-vKX_x"
   },
   "outputs": [],
   "source": [
    "def att_masking(input_ids):\n",
    "  attention_masks = []\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)\n",
    "  return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 6876,
     "status": "ok",
     "timestamp": 1703324596384,
     "user": {
      "displayName": "Chaganti Ramireddy",
      "userId": "07233275673922063388"
     },
     "user_tz": -330
    },
    "id": "TDFGL8XPKZ86"
   },
   "outputs": [],
   "source": [
    "train_attention_masks = att_masking(train_input_ids)\n",
    "validation_attention_masks = att_masking(validation_input_ids)\n",
    "\n",
    "train_labels = train_set['label'].to_numpy().astype('int')\n",
    "validation_labels = validation_set['label'].to_numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j1UJt0WJKfAR"
   },
   "outputs": [],
   "source": [
    "train_inputs = train_input_ids\n",
    "validation_inputs = validation_input_ids\n",
    "train_masks = train_attention_masks\n",
    "validation_masks = validation_attention_masks\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QNuE9Tl2KwMN"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9_p_vSKfK3cN"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = XLMForSequenceClassification.from_pretrained(\"xlm-roberta-large\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BeK69C6xX5mw"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.load(f\"train_xlm-roberta-large_dataloader_batch_size_8.pt\")\n",
    "validation_dataloader = torch.load(f\"validation_xlm-roberta-large_dataloader_batch_size_8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nyr_rKJDKqX1"
   },
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "max_grad_norm = 1.0\n",
    "epochs = 15\n",
    "num_total_steps = len(train_dataloader)*epochs\n",
    "num_warmup_steps = 1000\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "seed_val = 2212\n",
    "\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_4OhQvLIKuv6"
   },
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "wandb.init(\n",
    "        project=f\"{model_type}\",\n",
    "        name=f\"{model_name}_L{lr}_B{batch_size}_E{epochs}\",\n",
    "\n",
    "        config={\n",
    "        \"architecture\": model_name,\n",
    "        \"dataset\": \"ILDC\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"token_length\": 512,\n",
    "        \"model_name\": model_name\n",
    "        }\n",
    "    )\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        avg_eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    macro_f1 = f1_score(pred_flat, labels_flat, average='macro')\n",
    "    micro_f1 = f1_score(pred_flat, labels_flat, average='micro')\n",
    "    accuracy = accuracy_score(pred_flat, labels_flat)\n",
    "    precision = precision_score(pred_flat, labels_flat)\n",
    "    recall = recall_score(pred_flat, labels_flat)\n",
    "    confusion = confusion_matrix(labels_flat, pred_flat)\n",
    "    epoch_metrics = {\n",
    "        'epoch': epoch_i,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        \"flat_accuracy\":avg_eval_accuracy,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"Avg Train Loss\": avg_train_loss,\n",
    "        'confusion_matrix': confusion.tolist()\n",
    "    }\n",
    "    print(f\"epoch={epoch_i}, macro_f1: {macro_f1}, micro_f1: {micro_f1}, accuracy={accuracy}, precision: {precision}, recall: {recall}, loss={loss}\")\n",
    "    checkpoint_folder = f\"./Transformers_GPU/{model_type}/{model_name}_L{lr}_E{epochs}_B{batch_size}\"\n",
    "    epochs_folder = f\"{checkpoint_folder}/epochs\"\n",
    "    if not os.path.exists(epochs_folder):\n",
    "        os.makedirs(epochs_folder)\n",
    "\n",
    "    with open(f'{epochs_folder}/epoch{epoch_i}_metrics.json', 'w') as json_file:\n",
    "        json.dump(epoch_metrics, json_file, indent=4)\n",
    "\n",
    "    print(f\"epoch_{epoch_i}_metrics.json saved to {epochs_folder}\\n\")\n",
    "\n",
    "    wandb.log({\"Flat Accuracy\":avg_eval_accuracy, \"Accuracy\":accuracy , \"Macro_f1\":macro_f1,\"Micro_f1\":micro_f1, \"Precision\":precision, \"Recall\":recall, \"Avg Train Loss\": avg_train_loss})\n",
    "\n",
    "    print(f\"epoch_{epoch_i} logging is done...\\n\")\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Now Training for {model_name} is Completed.......\\n\")\n",
    "\n",
    "print(\"Saving model to %s\\n\" % checkpoint_folder)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(checkpoint_folder)\n",
    "tokenizer.save_pretrained(checkpoint_folder)\n",
    "\n",
    "print(\"Model Saved to %s\\n\" % checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, f\"{checkpoint_folder}/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
